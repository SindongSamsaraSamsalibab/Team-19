{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb749750-4bd2-40c2-a14a-1036f302944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-09 13:04:13.483928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744196653.502495   19298 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744196653.508251   19298 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744196653.523370   19298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744196653.523389   19298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744196653.523391   19298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744196653.523393   19298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 13:04:13.529000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Lab1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import nltk\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d478a8-943c-4b6e-990b-f370b32a8475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1325ee13-b828-4a06-a352-59d8a83724f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mohamedbakhet/amazon-books-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "#PATH\n",
    "filepath = '/root/.cache/kagglehub/datasets/mohamedbakhet/amazon-books-reviews/versions/1/Books_rating.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584d3010-ef5f-4c8b-9e2f-7c6171835502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>1882931173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_id</th>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profileName</th>\n",
       "      <td>Jim of Oz \"jim-of-oz\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review/helpfulness</th>\n",
       "      <td>7/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review/score</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review/time</th>\n",
       "      <td>940636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review/summary</th>\n",
       "      <td>Nice collection of Julie Strain images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review/text</th>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    0\n",
       "Id                                                         1882931173\n",
       "Title                                  Its Only Art If Its Well Hung!\n",
       "Price                                                             NaN\n",
       "User_id                                                 AVCGYZL8FQQTD\n",
       "profileName                                     Jim of Oz \"jim-of-oz\"\n",
       "review/helpfulness                                                7/7\n",
       "review/score                                                      4.0\n",
       "review/time                                                 940636800\n",
       "review/summary                 Nice collection of Julie Strain images\n",
       "review/text         This is only for Julie Strain fans. It's a col..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=pd.read_csv(filepath)\n",
    "print(len(data))\n",
    "display(data[0:1].T)\n",
    "data=data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65539c0c-6e25-4021-986f-537ae9e62e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "inputt=data['review/text']\n",
    "labelt=data['review/score']\n",
    "\n",
    "print(len(inputt))\n",
    "print(labelt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f28be7-667f-4a2b-9343-af0306989278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont care much for dr seuss but after reading philip nels book i changed my mindthats a good testimonial to the power of rels writing and thinking rel plays dr seuss the ultimate compliment of treating him as a serious poet as well as one of the th centurys most interesting visual artists and after reading his book i decided that a trip to the mandeville collections of the library at university of california in san diego was in order so i could visit some of the incredible seussgeisel holdings they have theretheres almost too much to take in for like william butler yeats seuss led a career that constantly shifted and metamoprhized itself to meet new historical and political cirsumstances so he seems to have been both a leftist and a conservative at different junctures of his career both in politics and in art as nel shows us he was once a cartoonist for the fabled pm magazine and like andy warhol he served his time slaving in the ad business too all was in the service of amusing and broadening the minds of us children nel doesnt hesitate to administer a sound spanking to the seuss industry that since his death has seen fit to license all kinds of awful products including the recent cat in the hat film with mike myers oh what a catastrophethe book is great and i can especially recommend the work of the picture editor who has given us a bounty of good illustrations\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentences = []\n",
    "labels = []\n",
    "for i, text in enumerate(inputt):\n",
    "    if not isinstance(text, str):  # Convert non-strings to empty string\n",
    "        text = str(text) if text is not None else ''\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', text)  # Remove emails\n",
    "    text = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', text)  # Remove IPs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', '', text) #Remove numbers\n",
    "    sentences.append(text)\n",
    "\n",
    "labels = [int(round(x)) for x in labelt]\n",
    "\n",
    "print(sentences[1])\n",
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b43532a-eaa3-4926-b783-074d842421ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "num_labels= 6\n",
    "\n",
    "# Load the tokenizer for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',num_labels=num_labels)\n",
    "print(len(sentences))\n",
    "# Tokenize the sentences\n",
    "encoding = tokenizer(\n",
    "    sentences,                      # List of sentences\n",
    "    add_special_tokens=True,        # Add [CLS] and [SEP] tokens\n",
    "    max_length=128,                 # Truncate/pad to this max length\n",
    "    padding='max_length',           # Pad all sentences to the same length\n",
    "    truncation=True,                # Truncate sentences longer than max_length\n",
    "    return_tensors=\"pt\"             # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = encoding['input_ids']            # Shape: [num_sentences, max_length]\n",
    "attention_mask = encoding['attention_mask']  # Shape: [num_sentences, max_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71914655-00d6-452c-94a0-d5c035623a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268ba22e-30aa-453e-81f6-2c96ab240d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = torch.tensor(labels)  # Shape: [num_sentences]\n",
    "print(len(input_ids))\n",
    "#print(labels[0:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f49e2f-7827-44d7-8d51-f3b1c484e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "print(len(dataset))\n",
    "\n",
    "# Specify dataset sizes (e.g., 70% train, 15% validation, 15% test)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Randomly split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "batch_s = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_s, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_s)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3b1fc1-2daa-4f4d-a659-34d5dba866fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import BertModel\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2065f5-1cbe-448c-a44a-652c944bf1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Returns True if a GPU is available\n",
    "# Specify the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move your model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Move your data tensors to the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a052e8f4-1338-4b14-a349-c878d1c6cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "best_loss = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ad9c56-fde2-4bb5-936f-08ee357c3188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwilwoh-2\u001b[0m (\u001b[33mertveh-4-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Lab1/wandb/run-20250409_131458-5o3lxxy2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group_19_lab1/runs/5o3lxxy2' target=\"_blank\">devout-dawn-10</a></strong> to <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group_19_lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group_19_lab1' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/Group_19_lab1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group_19_lab1/runs/5o3lxxy2' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/Group_19_lab1/runs/5o3lxxy2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"Group_19_lab1\")\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": learning_rate\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ca517c-e7f2-469e-98f0-ea9bf75ec338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Train loss:  0.8083270762585635 Val loss:  0.7436480961994014\n",
      "Epoch:  1 Train loss:  0.6990283356978354 Val loss:  0.7201673224229704\n",
      "Epoch:  2 Train loss:  0.632366037446867 Val loss:  0.7433671588924798\n",
      "Epoch:  3 Train loss:  0.5639021888257235 Val loss:  0.7639821552756157\n",
      "Epoch:  4 Train loss:  0.49503342294696623 Val loss:  0.7776042968458072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_loss = 10000\n",
    "for epoch in range(epochs):\n",
    "    avg_train_loss = 0\n",
    "    avg_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device)           # Move input_ids to GPU/CPU\n",
    "        attention_mask = batch[1].to(device)      # Move attention_mask to GPU/CPU\n",
    "        labels = batch[2].to(device)              # Move labels to GPU/CPU\n",
    "        output = model(input_ids,attention_mask=attention_mask)\n",
    "        output = output.logits\n",
    "\n",
    "        loss = loss_fn(output,labels)\n",
    "        avg_train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[0].to(device)           # Move input_ids to GPU/CPU\n",
    "            attention_mask = batch[1].to(device)      # Move attention_mask to GPU/CPU\n",
    "            labels = batch[2].to(device)              # Move labels to GPU/CPU\n",
    "            output = model(input_ids,attention_mask=attention_mask)\n",
    "            output = output.logits\n",
    "            loss = loss_fn(output,labels)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model = model\n",
    "            avg_val_loss += loss.item()\n",
    "            for i in range(len(output)):\n",
    "                o = torch.argmax(output[i]).item()\n",
    "                label = labels[i].item()\n",
    "                total += 1\n",
    "                if  o == label:\n",
    "                    correct += 1\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "        \n",
    "    avg_train_loss = avg_train_loss/len(train_loader)\n",
    "    avg_val_loss = avg_val_loss/len(val_loader)\n",
    "    val_acc = correct/total\n",
    "    wandb.log({\n",
    "        \"train_loss\":avg_train_loss,\n",
    "        \"val_loss\":avg_val_loss,\n",
    "        \"val_acc\":val_acc\n",
    "    })\n",
    "    del input_ids, attention_mask, labels, output, loss\n",
    "\n",
    "    \n",
    "    print(\"Epoch: \",epoch,\"Train loss: \",avg_train_loss,\"Val loss: \", avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba6c3b07-23a8-4904-aced-05d63c80d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7179333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "best_model.eval()\n",
    "best_model.to(device)\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[0].to(device)           # Move input_ids to GPU/CPU\n",
    "        attention_mask = batch[1].to(device)      # Move attention_mask to GPU/CPU\n",
    "        labels = batch[2].to(device)              # Move labels to GPU/CPU\n",
    "        output = best_model(input_ids,attention_mask=attention_mask)\n",
    "        output = output.logits\n",
    "        \n",
    "        for i in range(len(output)):\n",
    "            o = torch.argmax(output[i]).item()\n",
    "            label = labels[i].item()\n",
    "            total += 1\n",
    "            if  o == label:\n",
    "                correct += 1\n",
    "print(correct/total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c4e920-ca3a-47f4-96cc-5f6cd0a36e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#torch.save(best_model.state_dict(), \"model_name\")\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec2a61-9530-4395-8cf2-3bbdc0c88c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
