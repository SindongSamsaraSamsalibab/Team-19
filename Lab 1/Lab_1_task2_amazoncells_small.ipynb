{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb749750-4bd2-40c2-a14a-1036f302944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Lab1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-14 14:21:51.206005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744633311.230456   53465 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744633311.237745   53465 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744633311.256106   53465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744633311.256128   53465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744633311.256133   53465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744633311.256138   53465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-14 14:21:51.263368: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Lab1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import os\n",
    "import nltk\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')\n",
    "print(os.getcwd())\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')\n",
    "print(os.getcwd())\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "with open(\"amazon_cells_labelled.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        # Strip whitespace, split the sentence and label\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            line = line.replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '')\n",
    "            line = line.replace('[^\\w\\s]','')\n",
    "            line = line.replace('\\d', '') \n",
    "            line = line.replace('\\t', '') \n",
    "            labels.append(int(line[-1]))\n",
    "            sentences.append(line[:len(line)-1])\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b43532a-eaa3-4926-b783-074d842421ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "\n",
    "# Tokenize the sentences\n",
    "encoding = tokenizer(\n",
    "    sentences,                      # List of sentences\n",
    "    add_special_tokens=True,        # Add [CLS] and [SEP] tokens\n",
    "    max_length=128,                 # Truncate/pad to this max length\n",
    "    padding='max_length',           # Pad all sentences to the same length\n",
    "    truncation=True,                # Truncate sentences longer than max_length\n",
    "    return_tensors=\"pt\"             # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = encoding['input_ids']            # Shape: [num_sentences, max_length]\n",
    "attention_mask = encoding['attention_mask']  # Shape: [num_sentences, max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268ba22e-30aa-453e-81f6-2c96ab240d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert labels to a PyTorch tensor\n",
    "labels = torch.tensor(labels)  # Shape: [num_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f49e2f-7827-44d7-8d51-f3b1c484e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "print(len(dataset))\n",
    "\n",
    "# Specify dataset sizes (e.g., 70% train, 15% validation, 15% test)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Randomly split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "batch_s = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_s, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_s)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75a80ad-b8ca-4471-b16b-81ab1e8c7828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import BertModel\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "#model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2065f5-1cbe-448c-a44a-652c944bf1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Returns True if a GPU is available\n",
    "# Specify the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move your model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Move your data tensors to the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ca517c-e7f2-469e-98f0-ea9bf75ec338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Train loss:  0.6008738137104295 Val loss:  0.4376864582300186\n",
      "Epoch:  1 Train loss:  0.2947336401451718 Val loss:  0.2431981161236763\n",
      "Epoch:  2 Train loss:  0.1429926950315183 Val loss:  0.23353499844670295\n",
      "Epoch:  3 Train loss:  0.0711672600274059 Val loss:  0.22837235275655984\n",
      "Epoch:  4 Train loss:  0.06539744985374538 Val loss:  0.23265056516975163\n",
      "Epoch:  5 Train loss:  0.04597081049260768 Val loss:  0.25546308448538185\n",
      "Epoch:  6 Train loss:  0.050173930887302216 Val loss:  0.2952569654211402\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=0)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "best_loss = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_train_loss = 0\n",
    "    avg_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device)           # Move input_ids to GPU/CPU\n",
    "        attention_mask = batch[1].to(device)      # Move attention_mask to GPU/CPU\n",
    "        labels = batch[2].to(device)              # Move labels to GPU/CPU\n",
    "        output = model(input_ids,attention_mask=attention_mask)\n",
    "        output = output.logits\n",
    "\n",
    "        loss = loss_fn(output,labels)\n",
    "        avg_train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[0].to(device)           # Move input_ids to GPU/CPU\n",
    "            attention_mask = batch[1].to(device)      # Move attention_mask to GPU/CPU\n",
    "            labels = batch[2].to(device)              # Move labels to GPU/CPU\n",
    "            output = model(input_ids,attention_mask=attention_mask)\n",
    "            output = output.logits\n",
    "            loss = loss_fn(output,labels)\n",
    "            avg_val_loss += loss.item()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_train_loss = avg_train_loss/len(train_loader)\n",
    "    avg_val_loss = avg_val_loss/len(val_loader)\n",
    "    del output, loss\n",
    "\n",
    "    \n",
    "    print(\"Epoch: \",epoch,\"Train loss: \",avg_train_loss,\"Val loss: \", avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6c3b07-23a8-4904-aced-05d63c80d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "for batch in val_loader:\n",
    "    input_ids = batch[0].to(device)           # Move input_ids to GPU/CPU\n",
    "    attention_mask = batch[1].to(device)      # Move attention_mask to GPU/CPU\n",
    "    labels = batch[2].to(device)              # Move labels to GPU/CPU\n",
    "    output = model(input_ids,attention_mask=attention_mask)\n",
    "    output = output.logits\n",
    "    for i in range(len(output)):\n",
    "        o = torch.argmax(output[i]).item()\n",
    "        label = labels[i].item()\n",
    "        total += 1\n",
    "        if  o == label:\n",
    "            correct += 1\n",
    "print(correct/total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4e920-ca3a-47f4-96cc-5f6cd0a36e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
