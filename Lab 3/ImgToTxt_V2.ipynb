{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46407acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0d43af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-brook-13</strong> at: <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning/runs/mrf7bbpp' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning/runs/mrf7bbpp</a><br> View project at: <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250508_165629-mrf7bbpp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Skola\\D7047e\\Lab3\\wandb\\run-20250508_174928-19sn030u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning/runs/19sn030u' target=\"_blank\">flowing-paper-14</a></strong> to <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning/runs/19sn030u' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/Group19_Lab3_flickr30k-captioning/runs/19sn030u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Config\n",
    "wandb.init(project=\"Group19_Lab3_flickr30k-captioning\")\n",
    "config = wandb.config\n",
    "config.batch_size = 64\n",
    "config.embed_size = 256\n",
    "config.hidden_size = 512\n",
    "config.num_layers = 1\n",
    "config.learning_rate = 3e-4\n",
    "config.num_epochs = 5\n",
    "config.max_len = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5eeb5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_name', ' comment_number', ' comment']\n",
      "torch.Size([3, 224, 224]) tensor([ 1,  4, 28, 17, 29, 30, 23, 31, 17, 32, 19, 20,  2,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "torch.Size([3, 224, 224]) tensor([ 1, 60, 28, 53, 61, 55, 32, 62, 63, 20,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "torch.Size([3, 224, 224]) tensor([ 1, 32, 77, 78, 71, 18, 73, 83, 84, 85, 20,  2,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "torch.Size([3, 224, 224]) tensor([ 1, 32, 33, 53, 32, 94, 96, 32, 93,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "torch.Size([3, 224, 224]) tensor([  1,   4,  28,  23, 104,  32, 111,  20,   2,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Dataset and Preprocessing ---\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #index to string\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}             #string to index\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            tokens = word_tokenize(sentence.lower())\n",
    "            frequencies.update(tokens)\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        return [self.stoi.get(word, self.stoi[\"<UNK>\"]) for word in word_tokenize(text.lower())]\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform, vocab, max_len=30):\n",
    "        self.max_len = max_len\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Load CSV and group comments by image\n",
    "        df = pd.read_csv(captions_file, delimiter='|', encoding='utf-8', quotechar='\"', escapechar='\\\\')\n",
    "        df.dropna(inplace=True)\n",
    "        df.columns.str.strip() \n",
    "        print(df.columns.tolist())\n",
    "        # Group all captions per image\n",
    "        self.captions_dict = df.groupby(\"image_name\")[' comment'].apply(list).to_dict()\n",
    "        self.image_names = list(self.captions_dict.keys())\n",
    "\n",
    "        # Build vocab using all captions\n",
    "        all_captions = [caption for captions in self.captions_dict.values() for caption in captions]\n",
    "        self.vocab.build_vocabulary(all_captions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.root_dir, image_name)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except:\n",
    "            print(f\"Error loading image: {image_path}, {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Sample a random caption for this image\n",
    "        captions = self.captions_dict[image_name]\n",
    "        caption = random.choice(captions)\n",
    "\n",
    "        # Convert caption to tensor of word indices\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        # Pad the caption to max_len\n",
    "\n",
    "        if len(numericalized_caption) < self.max_len:\n",
    "            numericalized_caption += [self.vocab.stoi[\"<PAD>\"]] * (self.max_len - len(numericalized_caption))\n",
    "        else:\n",
    "            numericalized_caption = numericalized_caption[:self.max_len]\n",
    "        \n",
    "        \n",
    "\n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "\n",
    "def collate_fn(batch, pad_idx):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for img, cap in batch:\n",
    "        images.append(img)\n",
    "        captions.append(cap)\n",
    "    captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=pad_idx)\n",
    "    return torch.stack(images), captions\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "img_dir = r\"C:\\Skola\\D7047e\\Lab3\\flickr30k_images\\flickr30k_images\\flickr30k_images\"\n",
    "cap_dir = r\"c:\\Skola\\D7047e\\Lab3\\flickr30k_images\\flickr30k_images\\results_clean.csv\"\n",
    "\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "dataset = FlickrDataset(root_dir=img_dir, captions_file=cap_dir, transform=transform, vocab=vocab)\n",
    "\n",
    "for i in range(5):\n",
    "    img, cap = dataset[i]\n",
    "    print(img.shape, cap)\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [.8, .1, .1])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, vocab.stoi[\"<PAD>\"]))\n",
    "val_loader = DataLoader(val_data, batch_size=config.batch_size, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab.stoi[\"<PAD>\"]))\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=lambda x: collate_fn(x, vocab.stoi[\"<PAD>\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Encoder and Decoder ---\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])\n",
    "        features = features.unsqueeze(1)\n",
    "        inputs = torch.cat((features, embeddings), 1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oscar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Oscar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Training ---\n",
    "encoder = EncoderCNN(config.embed_size).to(device)\n",
    "decoder = DecoderRNN(config.embed_size, config.hidden_size, len(vocab.stoi), config.num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "params = list(decoder.parameters()) + list(encoder.resnet.fc.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=config.learning_rate)\n",
    "\n",
    "\n",
    "def train():\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for epoch in range(config.num_epochs):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        print(\"We are on this epoch:\",epoch)\n",
    "        for imgs, caps in loop:\n",
    "            print(\"Bitch\")\n",
    "            print(\"on this imgs:\",imgs.size())\n",
    "            imgs, caps = imgs.to(device), caps.to(device)\n",
    "            features = encoder(imgs)\n",
    "            outputs = decoder(features, caps)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(2)), caps[:, 1:].reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(total_loss)\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": total_loss / len(train_loader)})\n",
    "        print(f\"\\nEpoch {epoch}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        evaluate(val_loader, encoder, decoder)\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate(loader, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_bleu = 0\n",
    "    loop = tqdm(loader, desc=\"Evaluating\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, caps in loop:\n",
    "            imgs = imgs.to(device)\n",
    "            features = encoder(imgs)\n",
    "            output_ids = generate_caption(decoder, features, vocab)\n",
    "\n",
    "            reference = [word_tokenize(vocab.itos[idx.item()]) for idx in caps[0] if idx.item() not in [vocab.stoi[\"<PAD>\"], vocab.stoi[\"<SOS>\"]]]\n",
    "            candidate = [vocab.itos[idx] for idx in output_ids if idx not in [vocab.stoi[\"<PAD>\"], vocab.stoi[\"<SOS>\"], vocab.stoi[\"<EOS>\"]]]\n",
    "\n",
    "            bleu = sentence_bleu([reference], candidate, weights=(0.5, 0.5))\n",
    "            total_bleu += bleu\n",
    "            loop.set_postfix(bleu=bleu)\n",
    "\n",
    "    avg_bleu = total_bleu / len(loader)\n",
    "    wandb.log({\"val_bleu\": avg_bleu})\n",
    "    print(f\"\\nValidation BLEU score: {avg_bleu:.4f}\")\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "\n",
    "def generate_caption(decoder, feature, vocab, max_len=20):\n",
    "    result = []\n",
    "    input = feature.unsqueeze(1)\n",
    "    states = None\n",
    "\n",
    "    for _ in tqdm(range(max_len), desc=\"Generating Caption\", leave=False):\n",
    "        hiddens, states = decoder.lstm(input, states)\n",
    "        output = decoder.linear(hiddens.squeeze(1))\n",
    "        predicted = output.argmax(1)\n",
    "        result.append(predicted.item())\n",
    "        input = decoder.embed(predicted).unsqueeze(1)\n",
    "        if predicted.item() == vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 25427\n",
      "Train loader batches: 398\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Train loader batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1c72d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/398 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on this epoch: 0\n",
      "Bitch\n",
      "on this imgs: torch.Size([64, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/398 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1920) to match target batch_size (1856).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m features = encoder(imgs)\n\u001b[32m     23\u001b[39m outputs = decoder(features, caps)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m optimizer.zero_grad()\n\u001b[32m     27\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\loss.py:1295\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (1920) to match target batch_size (1856)."
     ]
    }
   ],
   "source": [
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
