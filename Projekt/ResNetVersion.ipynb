{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29370dfd-e3e3-44d6-a74c-ea3f95b9073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprossesing\n",
    "# Read the picture files (stored in data folder).\n",
    "# Decode the JPEG content to RGB grids of pixels with channels.\n",
    "# Convert these into floating-point tensors for input to neural nets.\n",
    "# Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as training neural networks with this range gets efficient).\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from PIL import ImageOps\n",
    "\n",
    "# load and display an image with Matplotlib\n",
    "train_dataset = torchvision.datasets.ImageFolder(root= r'C:\\Users\\elias\\Documents\\Neural networks\\Advanced\\Project\\cross_out_dataset\\train\\images')\n",
    "valid_dataset = torchvision.datasets.ImageFolder(root= r'C:\\Users\\elias\\Documents\\Neural networks\\Advanced\\Project\\cross_out_dataset\\val\\images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8805c3ec-4b37-4f10-a6f9-66748454b642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: eliassailekarlsson (eliassailekarlsson-lule-university-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\elias\\Documents\\Neural networks\\Advanced\\Project\\wandb\\run-20250426_035400-dfd4j4ps</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eliassailekarlsson-lule-university-of-technology/ResNet/runs/dfd4j4ps' target=\"_blank\">rural-butterfly-12</a></strong> to <a href='https://wandb.ai/eliassailekarlsson-lule-university-of-technology/ResNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eliassailekarlsson-lule-university-of-technology/ResNet' target=\"_blank\">https://wandb.ai/eliassailekarlsson-lule-university-of-technology/ResNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eliassailekarlsson-lule-university-of-technology/ResNet/runs/dfd4j4ps' target=\"_blank\">https://wandb.ai/eliassailekarlsson-lule-university-of-technology/ResNet/runs/dfd4j4ps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"eliassailekarlsson-lule-university-of-technology\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"ResNet\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": 0.0002,\n",
    "        \"architecture\": \"ResNet\",\n",
    "        \"dataset\": \"cross_out_dataset\",\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6987ec0-511e-4ecf-ad48-7b2d7936e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inproved\n",
    "def pad_or_truncate_tensor(tensor, target_shape, position=\"leftUp\"):\n",
    "    \"\"\"\n",
    "    Pads or truncates a tensor to the specified target shape,\n",
    "    aligning the original content according to `position`.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor (2D or more).\n",
    "        target_shape (tuple): Desired shape (must match the last two dims).\n",
    "        position (str): One of 'leftUp', 'rightUp', 'leftDown', 'rightDown', 'center'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded or truncated tensor.\n",
    "    \"\"\"\n",
    "    orig_shape = tensor.shape[-2:]\n",
    "    padded_tensor = torch.zeros(*tensor.shape[:-2], *target_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "\n",
    "    crop_h = min(orig_shape[0], target_shape[0])\n",
    "    crop_w = min(orig_shape[1], target_shape[1])\n",
    "\n",
    "    # Determine input crop start\n",
    "    if position == \"leftUp\":\n",
    "        in_start_y, in_start_x = 0, 0\n",
    "        out_start_y, out_start_x = 0, 0\n",
    "    elif position == \"rightUp\":\n",
    "        in_start_y = 0\n",
    "        in_start_x = max(0, orig_shape[1] - crop_w)\n",
    "        out_start_y = 0\n",
    "        out_start_x = max(0, target_shape[1] - crop_w)\n",
    "    elif position == \"leftDown\":\n",
    "        in_start_y = max(0, orig_shape[0] - crop_h)\n",
    "        in_start_x = 0\n",
    "        out_start_y = max(0, target_shape[0] - crop_h)\n",
    "        out_start_x = 0\n",
    "    elif position == \"rightDown\":\n",
    "        in_start_y = max(0, orig_shape[0] - crop_h)\n",
    "        in_start_x = max(0, orig_shape[1] - crop_w)\n",
    "        out_start_y = max(0, target_shape[0] - crop_h)\n",
    "        out_start_x = max(0, target_shape[1] - crop_w)\n",
    "    elif position == \"center\":\n",
    "        in_start_y = max(0, (orig_shape[0] - crop_h) // 2)\n",
    "        in_start_x = max(0, (orig_shape[1] - crop_w) // 2)\n",
    "        out_start_y = max(0, (target_shape[0] - crop_h) // 2)\n",
    "        out_start_x = max(0, (target_shape[1] - crop_w) // 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown position: {position}\")\n",
    "\n",
    "    # Crop the input\n",
    "    cropped = tensor[..., in_start_y:in_start_y + crop_h, in_start_x:in_start_x + crop_w]\n",
    "\n",
    "    # Insert it in the correct position in the output tensor\n",
    "    padded_tensor[..., out_start_y:out_start_y + crop_h, out_start_x:out_start_x + crop_w] = cropped\n",
    "\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70541a66-3686-4635-99ba-1d1e4f2f4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPadCropTransform:\n",
    "    def __init__(self, target_size, position=\"center\"):\n",
    "        self.target_size = target_size\n",
    "        self.position = position\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return pad_or_truncate_tensor(tensor, self.target_size, self.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b33cb9-c4f9-4811-9737-6b2c40416bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryThreshold:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        \"\"\"\n",
    "        threshold: value between 0 and 1. Pixels above become 1, below become 0.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return (tensor > self.threshold).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c95e4c-bbcc-4785-9b81-9c5564bad0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SobelEdgeDetection:\n",
    "    def __init__(self):\n",
    "        # Define Sobel kernels\n",
    "        self.kernel_x = torch.tensor([[-1., 0., 1.],\n",
    "                                      [-2., 0., 2.],\n",
    "                                      [-1., 0., 1.]]).view(1, 1, 3, 3)\n",
    "        self.kernel_y = torch.tensor([[-1., -2., -1.],\n",
    "                                      [ 0.,  0.,  0.],\n",
    "                                      [ 1.,  2.,  1.]]).view(1, 1, 3, 3)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if tensor.dim() == 3 and tensor.shape[0] == 3:\n",
    "            # Convert RGB to grayscale\n",
    "            tensor = 0.2989 * tensor[0] + 0.5870 * tensor[1] + 0.1140 * tensor[2]\n",
    "            tensor = tensor.unsqueeze(0)  # Add channel dim back\n",
    "\n",
    "        tensor = tensor.unsqueeze(0)  # Add batch dim\n",
    "\n",
    "        # Apply filters (assume grayscale image [1, 1, H, W])\n",
    "        edge_x = F.conv2d(tensor, self.kernel_x, padding=1)\n",
    "        edge_y = F.conv2d(tensor, self.kernel_y, padding=1)\n",
    "\n",
    "        # Compute edge magnitude\n",
    "        edge = torch.sqrt(edge_x ** 2 + edge_y ** 2)\n",
    "\n",
    "        # Remove batch/channel dims\n",
    "        edge = edge.squeeze(0)\n",
    "\n",
    "        # Normalize to [0, 1] range\n",
    "        edge = (edge - edge.min()) / (edge.max() - edge.min() + 1e-6)\n",
    "\n",
    "        return edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475ab2c7-0dd2-4a57-9a5e-76ad1ee603e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeAndPadToFixed:\n",
    "    def __init__(self, size, fill_color=0):\n",
    "        self.target_w, self.target_h = size\n",
    "        self.fill_color = fill_color\n",
    "\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "\n",
    "        # Resize with aspect ratio preserved\n",
    "        scale = min(self.target_w / w, self.target_h / h)\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        image = image.resize((new_w, new_h))\n",
    "\n",
    "        # Pad to target size\n",
    "        pad_w = self.target_w - new_w\n",
    "        pad_h = self.target_h - new_h\n",
    "        left = random.randint(0, pad_w) if pad_w > 0 else 0\n",
    "        top = random.randint(0, pad_h) if pad_h > 0 else 0\n",
    "        right = pad_w - left\n",
    "        bottom = pad_h - top\n",
    "\n",
    "        return ImageOps.expand(image, border=(left, top, right, bottom), fill=self.fill_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92600019-d5f0-48be-83f8-c50ffe3fa532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayToRGB:\n",
    "    def __call__(self, tensor):\n",
    "        return tensor.expand(3, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619d794f-bcfc-41bf-b262-8bf6ff947a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLEAN', 'CROSS', 'DIAGONAL', 'DOUBLE_LINE', 'SCRATCH', 'SINGLE_LINE', 'WAVE', 'ZIG_ZAG']\n",
      "['CLEAN', 'CROSS', 'DIAGONAL', 'DOUBLE_LINE', 'SCRATCH', 'SINGLE_LINE', 'WAVE', 'ZIG_ZAG']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.classes)\n",
    "print(valid_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93b2716-de5c-4c61-a971-8799b47eedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04682f7c-07ea-4164-9107-18a601cb1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize 224x224\n",
    "train_transform = transforms.Compose([\n",
    "    ResizeAndPadToFixed((160, 80)),  # Guarantees fixed size (W=160, H=80)\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    # transforms.RandomRotation(degrees=(30, 70)),\n",
    "    transforms.ToTensor(),\n",
    "    SobelEdgeDetection(),\n",
    "    GrayToRGB(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5],\n",
    "        std=[0.5]\n",
    "    )\n",
    "])\n",
    "# the validation transforms\n",
    "valid_transform = transforms.Compose([\n",
    "    ResizeAndPadToFixed((160, 80)),  # Guarantees fixed size (W=160, H=80)\n",
    "    transforms.ToTensor(),\n",
    "    SobelEdgeDetection(),\n",
    "    GrayToRGB(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5],\n",
    "        std=[0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "243b1281-9099-43e4-9537-6272accd8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\elias\\Documents\\Neural networks\\Advanced\\Project\\cross_out_dataset\\train\\images',\n",
    "    transform=train_transform\n",
    ")\n",
    "# validation dataset\n",
    "valid_dataset = datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\elias\\Documents\\Neural networks\\Advanced\\Project\\cross_out_dataset\\val\\images',\n",
    "    transform=valid_transform\n",
    ")\n",
    "# training data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "# validation data loaders\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafeaef-ce9f-4221-937a-8d60c95ded38",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa015307-cc42-45d0-b7f5-3ca19f03ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Replace the last layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29415e49-f024-489f-99f1-b65ec767c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26257ee-9c63-4270-9865-449388d89b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define datasets and loaders\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=r'path\\to\\train',\n",
    "        transform=train_transform\n",
    "    )\n",
    "    valid_dataset = datasets.ImageFolder(\n",
    "        root=r'path\\to\\val',\n",
    "        transform=valid_transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=4, pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = ...  # your model definition here\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = ...\n",
    "    criterion = ...\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_dataset)\n",
    "        val_loss /= len(valid_dataset)\n",
    "        val_acc = val_acc.double() / len(valid_dataset)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} \"\n",
    "              f\"Val Loss: {val_loss:.4f} \"\n",
    "              f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be9ec7-1381-495c-a1f6-9ff278c2b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    ResizeByLongerSide((160, 80)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Grayscale(),\n",
    "    #CustomPadCropTransform((128, 128), position=\"leftDown\"),\n",
    "    #SobelEdgeDetection(),\n",
    "    #BinaryThreshold(threshold=0.2),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c53d7-c663-4466-9a75-5e6d795a8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=r'C:\\Users\\elias\\Documents\\Neural networks\\Advanced\\Project\\cross_out_dataset\\train\\images', \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9f541-39ef-4ae1-8b37-496d720a629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img, label = dataset[290905]\n",
    "print(\"Image shape:\", img.shape)  # e.g. torch.Size([3, 128, 128])\n",
    "print(\"Label:\", label)\n",
    "\n",
    "# Optional: show image\n",
    "plt.imshow(img.permute(1, 2, 0))  # Convert from [C, H, W] to [H, W, C]\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302834a-042f-4098-8029-d22224b113f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the same dataset with different transforms\n",
    "#dataset1 = CIFAR10(root='./data', train=True, download=True, transform=transform1)\n",
    "#dataset2 = CIFAR10(root='./data', train=True, download=True, transform=transform2)\n",
    "\n",
    "# Combine datasets\n",
    "#combined_dataset = ConcatDataset([dataset1, dataset2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af92f6-3c0c-418e-b445-5b0b821bf011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abcb1f-e848-4c3e-81a9-1f58ed9d15ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
