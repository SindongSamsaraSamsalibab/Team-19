{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de920b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification with ResNet-18 and wandb Logging\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdc821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moscfah-1\u001b[0m (\u001b[33mertveh-4-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Oscar\\AppData\\Local\\Programs\\Microsoft VS Code\\wandb\\run-20250425_081412-xqxy4oxc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ertveh-4-lule-university-of-technology/handwritten-multiclass-resnet18/runs/xqxy4oxc' target=\"_blank\">eager-dragon-8</a></strong> to <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/handwritten-multiclass-resnet18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/handwritten-multiclass-resnet18' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/handwritten-multiclass-resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ertveh-4-lule-university-of-technology/handwritten-multiclass-resnet18/runs/xqxy4oxc' target=\"_blank\">https://wandb.ai/ertveh-4-lule-university-of-technology/handwritten-multiclass-resnet18/runs/xqxy4oxc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "lr=0.0001\n",
    "batch_size=32\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"handwritten-multiclass-resnet18\")\n",
    "\n",
    "# RandomPadToSize for preprocessing\n",
    "class RandomPadToSize:\n",
    "    def __init__(self, target_height, target_width):\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "        pad_left = (self.target_width - width) // 2\n",
    "        pad_top = (self.target_height - height) // 2\n",
    "        pad_right = self.target_width - width - pad_left\n",
    "        pad_bottom = self.target_height - height - pad_top\n",
    "        padding = (pad_left, pad_top, pad_right, pad_bottom)\n",
    "        return transforms.functional.pad(img, padding, fill=255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5885fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformations\n",
    "transformT = transforms.Compose([\n",
    "    RandomPadToSize(224, 224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.label_map = {\n",
    "            \"CLEAN\": 0,\n",
    "            \"CROSS\": 1,\n",
    "            \"DIAGONAL\": 2,\n",
    "            \"DOUBLE_LINE\": 3,\n",
    "            \"SCRATCH\": 4,\n",
    "            \"SINGLE_LINE\": 5,\n",
    "            \"WAVE\": 6,\n",
    "            \"ZIG_ZAG\": 7\n",
    "        }\n",
    "        valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\n",
    "        for label in self.label_map:\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(class_dir):\n",
    "                ext = os.path.splitext(fname)[1].lower()\n",
    "                if ext in valid_extensions:\n",
    "                    fpath = os.path.join(class_dir, fname)\n",
    "                    try:\n",
    "                        with Image.open(fpath) as img:\n",
    "                            if min(img.size) > 29:\n",
    "                                self.samples.append((fpath, self.label_map[label]))\n",
    "                    except Exception:\n",
    "                        print(f\"Warning: Skipping unreadable file during init: {fpath}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if min(image.size) <= 29:\n",
    "                raise ValueError(\"Image too small\")\n",
    "        except (UnidentifiedImageError, ValueError, OSError) as e:\n",
    "            print(f\"Warning: Skipping file {img_path} ({str(e)})\")\n",
    "            return self.__getitem__((idx + 1) % len(self.samples))  # Retry with next sample\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = HandwritingDataset(r\"C:\\Skola\\D7047e\\cross_out_dataset\\train\\images\", transform=transformT)\n",
    "val_dataset = HandwritingDataset(r\"C:\\Skola\\D7047e\\cross_out_dataset\\val\\images\", transform=transformT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "model = resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 8)  # 8 classes\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222b6e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Train Loss: 0.2473, Accuracy: 0.9010, F1: 0.9006 | Val Loss: 2.9068, Accuracy: 0.5916, F1: 0.5337\n",
      "Epoch [2/3] Train Loss: 0.0949, Accuracy: 0.9643, F1: 0.9643 | Val Loss: 0.6298, Accuracy: 0.8515, F1: 0.8615\n",
      "Epoch [3/3] Train Loss: 0.0637, Accuracy: 0.9766, F1: 0.9766 | Val Loss: 0.0503, Accuracy: 0.9812, F1: 0.9812\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop with validation\n",
    "def train(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "        epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        wandb.log({\n",
    "            \"Train Loss\": epoch_loss,\n",
    "            \"Train Accuracy\": epoch_acc,\n",
    "            \"Train F1\": epoch_f1,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_acc,\n",
    "            \"Validation F1\": val_f1,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\",\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, F1: {epoch_f1:.4f} |\",\n",
    "              f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "# Run training\n",
    "train(3)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"resnet18_multiclass.pth\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
